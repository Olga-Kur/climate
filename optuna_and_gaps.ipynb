{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlaZMaD/climate/blob/main/optuna_and_gaps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_h2eXSxHkT5"
      },
      "outputs": [],
      "source": [
        "!pip install catboost\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsxrOL5I3pyn"
      },
      "outputs": [],
      "source": [
        "from catboost.metrics import RMSE\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9GAMcL0YCUH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pylab as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "# from google.colab import drive\n",
        "import catboost\n",
        "import xgboost\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "import calendar\n",
        "import json\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFHOCiqBYF4_"
      },
      "outputs": [],
      "source": [
        "# !ls /content/drive/MyDrive/weather_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rCmymFO3XHi"
      },
      "outputs": [],
      "source": [
        "!gdown  1s5c7x3L6STewm7GBApVAEcqbYbTu45RO\n",
        "# !gdown 1NT0GrfbUPdB8psXPS1k-wiVwc6A3oxgl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpOn_UM_3zRC"
      },
      "outputs": [],
      "source": [
        "fileName = 'для поиска предикторов.xlsx'\n",
        "time = 'Date Time'\n",
        "target = 'NEE'\n",
        "targets = ['NEE', 'RECO', 'GPP', 'NEE_cor']\n",
        "use_sl = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqLheD8BPGNj"
      },
      "outputs": [],
      "source": [
        "training_variables = ['Ta', 'RH', 'Pa', 'VPD', 'P', 'SWin', 'SWout', 'LWin', 'LWout', 'Rn', 'WTD', 'Ts', 'SHF']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgsSP78H5xVj"
      },
      "outputs": [],
      "source": [
        "data_all = pd.read_excel(fileName, skiprows=lambda x: x==1, sheet_name=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kpMB7ESV-56"
      },
      "outputs": [],
      "source": [
        "print(data_all.keys())\n",
        "print(data_all['FYB15'].columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO6LkJBc51Xj"
      },
      "outputs": [],
      "source": [
        "# print(data.head())\n",
        "# print(data.columns)\n",
        "data_fyb = pd.concat([data_all[item] for item in ['FYB15', 'FYB16', 'FYB17', 'FYB18', 'FYB19', 'FYB20', 'FYB21', 'FYB22']], ignore_index=True)\n",
        "data_upo = pd.concat([data_all[item] for item in ['UPO12', 'UPO17']], ignore_index=True)\n",
        "data_muh = data_all['MUH']\n",
        "data_plt = data_all['PLT']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyuuGPd6GCWk"
      },
      "outputs": [],
      "source": [
        "data_dict = {}\n",
        "data_dict['MUH'] = data_muh\n",
        "data_dict['PLT'] = data_plt\n",
        "data_dict['UPO'] = data_upo\n",
        "data_dict['FYB'] = data_fyb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0912q3i_gC7o"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  for _, item in data_dict.items():\n",
        "    item.index = item[time]\n",
        "    item.resample('1D').mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HphkfoT68S4A"
      },
      "outputs": [],
      "source": [
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1# if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols = list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "      cols.append(df.shift(i))\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "      cols.append(df.shift(-i))\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "      agg.dropna(inplace=True)\n",
        "    return agg.values\n",
        "\n",
        "def sin_transformer(period):\n",
        "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
        "\n",
        "def cos_transformer(period):\n",
        "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
        "\n",
        "def add_time_features(data_f, time_col, year_sincos=True, hour_sincos=True):\n",
        "  data = data_f.copy()\n",
        "  data['day_of_year_f'] = data[time_col].dt.dayofyear\n",
        "  data['year_f'] = data[time_col].dt.year\n",
        "  years_in_dataset = data[time_col].dt.year.unique()\n",
        "  n_Days_in_year = {i:366 if calendar.isleap(i) else 365 for i in years_in_dataset}\n",
        "  new_features = []\n",
        "  if year_sincos:\n",
        "    new_features.append('year_sin')\n",
        "    new_features.append('year_cos')\n",
        "    for year in years_in_dataset:\n",
        "      data.loc[data['year_f']==year, \"year_sin\"] = sin_transformer(n_Days_in_year[year]).fit_transform(data[\"day_of_year_f\"])\n",
        "      data.loc[data['year_f']==year, \"year_cos\"] = cos_transformer(n_Days_in_year[year]).fit_transform(data[\"day_of_year_f\"])\n",
        "  if hour_sincos:\n",
        "    new_features.append('hour_sin')\n",
        "    new_features.append('hour_cos')\n",
        "    data['hour_sin'] = sin_transformer(24).fit_transform(data[time_col].dt.hour)\n",
        "    data['hour_cos'] = sin_transformer(24).fit_transform(data[time_col].dt.hour)\n",
        "\n",
        "  return data, new_features\n",
        "\n",
        "def data_preprocessing(data, time_col):\n",
        "  pass\n",
        "\n",
        "\n",
        "def get_best_params(data_f, target, features, time_col, sl=False, sl_length=6, name=\"\"):\n",
        "  data, new_features = add_time_features(data_f, time_col)\n",
        "  features = features + new_features\n",
        "  print('features: ', features)\n",
        "  data['process'] = np.invert(pd.isnull(data[target]))\n",
        "  if sl:\n",
        "    tmp_data = series_to_supervised(data[target], n_in=sl_length, dropnan=False)\n",
        "    new_cols = [f'target_{i}' for i in range(tmp_data.shape[1])]\n",
        "    new_sup_data = pd.DataFrame(tmp_data, columns=new_cols)\n",
        "    data = pd.concat([data, new_sup_data])\n",
        "    exist_cols = []\n",
        "    for col in  new_cols:\n",
        "      data[f'exists_{col}'] = np.invert(pd.isnull(data[col]))\n",
        "      exist_cols.append(f'exists_{col}')\n",
        "    features = features + new_cols\n",
        "\n",
        "  training_data = data.query('process==True').copy()\n",
        "  target_data = training_data[target].copy()\n",
        "  training_data[features].fillna(method='ffill', inplace=True)\n",
        "  training_data[features].fillna(0, inplace=True)\n",
        "\n",
        "  X_train, X_test, y_train, y_test  = train_test_split(training_data[features], training_data[target])\n",
        "\n",
        "  def objective(trial):\n",
        "    # define the grid\n",
        "\n",
        "    param = {\n",
        "        \"objective\": \"RMSE\",\n",
        "        \"iterations\":trial.suggest_int(\"iterations\", 100, 1500),\n",
        "        \"learning_rate\":trial.suggest_float('learning_rate',1e-3, 1.0, log=True),\n",
        "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-2, 1, log=True),\n",
        "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
        "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
        "        \"bootstrap_type\": trial.suggest_categorical(\n",
        "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
        "        ),\n",
        "        \"used_ram_limit\": \"6gb\",\n",
        "    }\n",
        "\n",
        "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
        "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
        "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
        "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
        "\n",
        "    bst = catboost.CatBoostRegressor(**param)\n",
        "    bst.fit(X_train, y_train)\n",
        "    preds = bst.predict(X_test)\n",
        "    pred_labels = np.rint(preds)\n",
        "    # objective should return the metrics that you want to optimize\n",
        "    accuracy = sklearn.metrics.mean_absolute_error(y_test, pred_labels)\n",
        "    return accuracy\n",
        "\n",
        "  study = optuna.create_study(direction=\"minimize\")\n",
        "  # you may increase the nubmer of trials in case you have enough time\n",
        "  study.optimize(objective, n_trials=10, timeout=600)\n",
        "\n",
        "  print(\"  Best value: {}\".format(study.best_trial.value))\n",
        "  print(study.best_params)\n",
        "  return study.best_params\n",
        "  # scaler = sklearn.preprocessing.StandardScaler()\n",
        "  # clf_X = scaler.fit_transform(training_data[features])\n",
        "  # clf_X = pd.DataFrame(clf_X, columns=features)\n",
        "  # clf_y = target_data\n",
        "  scores = sklearn.model_selection.cross_val_score(clf, clf_X, clf_y, cv=5, scoring='neg_mean_absolute_percentage_error')\n",
        "  print(scores, scores.mean())\n",
        "  clf_test = catboost.CatBoostRegressor(iterations=300,\n",
        "                                  learning_rate=0.03,\n",
        "                                  depth=10,\n",
        "                                  l2_leaf_reg=1,\n",
        "                                  grow_policy = 'Depthwise',\n",
        "                                  loss_function=RMSE())\n",
        "  clf_test.fit(clf_X, clf_y)\n",
        "\n",
        "  print(clf_test.feature_importances_)\n",
        "\n",
        "  feature_importance = clf_test.feature_importances_\n",
        "  sorted_idx = np.argsort(feature_importance)\n",
        "  fig = plt.figure(figsize=(12, 6))\n",
        "  plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
        "\n",
        "  if use_sl:\n",
        "    plt.yticks(range(len(sorted_idx)), np.array(features)[sorted_idx])\n",
        "  else:\n",
        "    plt.yticks(range(len(sorted_idx)), np.array(features)[sorted_idx])\n",
        "  plt.title(f'Feature Importance for {name}')\n",
        "  plt.savefig(f\"{name}.png\")\n",
        "  return {np.array(features)[i]:feature_importance[i] for i in sorted_idx}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ6MQU2f8qs5"
      },
      "outputs": [],
      "source": [
        "best_params = get_best_params(data_dict['FYB'], target, training_variables,  time)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_dict['FYB']\n",
        "features = training_variables\n",
        "data, new_features = add_time_features(data, time)\n",
        "features = features + new_features\n",
        "\n",
        "data['process'] = np.invert(pd.isnull(data[target]))\n",
        "training_data = data.query('process==True').copy()\n",
        "\n",
        "target_data = training_data[target].copy()\n",
        "\n",
        "training_data[features] = training_data[features].fillna(method='ffill', inplace=False)\n",
        "training_data[features].fillna(0, inplace=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test  = train_test_split(training_data[features], training_data[target])"
      ],
      "metadata": {
        "id": "mUJpIihX74wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWdLdqZTcOi1"
      },
      "outputs": [],
      "source": [
        "clf_test = catboost.CatBoostRegressor(**best_params)\n",
        "clf_test.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_predictions = clf_test.predict(X_test)\n",
        "print(sklearn.metrics.mean_squared_error(y_test, my_predictions))\n",
        "print(y_test.mean(), y_test.std())\n",
        "plt.hist(y_test-my_predictions, bins=50, range=(-5, 5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kS0teBLZ8WgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_df = pd.DataFrame({'y':y_test, 'error':y_test - my_predictions})\n",
        "\n",
        "# plot_df['bins'] = pd.cut(plot_df['y'], 50)\n",
        "\n",
        "# gp_data = plot_df.groupby('bins').mean()\n",
        "# plt.plot(gp_data['y'], gp_data['error'])\n",
        "\n",
        "# # plt.plot()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "veuEj6iLBNfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91dJOmOBcmaz"
      },
      "outputs": [],
      "source": [
        "print(clf_test.feature_importances_)\n",
        "\n",
        "feature_importance = clf_test.feature_importances_\n",
        "sorted_idx = np.argsort(feature_importance)\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
        "if use_sl:\n",
        "  plt.yticks(range(len(sorted_idx)), np.array(training_variables+exist_cols)[sorted_idx])\n",
        "else:\n",
        "  plt.yticks(range(len(sorted_idx)), np.array(features)[sorted_idx])\n",
        "plt.title('Feature Importance')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5W_LZb5IVpy"
      },
      "outputs": [],
      "source": [
        "data = data_dict['MUH']\n",
        "features = training_variables\n",
        "data, new_features = add_time_features(data, time)\n",
        "features = features + new_features\n",
        "\n",
        "data['process'] = np.invert(pd.isnull(data[target]))\n",
        "training_data = data.query('process==True').copy()\n",
        "\n",
        "target_data = training_data[target].copy()\n",
        "\n",
        "training_data[features] = training_data[features].fillna(method='ffill', inplace=False)\n",
        "training_data[features].fillna(0, inplace=True)\n",
        "\n",
        "clf_test = catboost.CatBoostRegressor(**best_params)\n",
        "clf_test.fit(training_data[features], training_data[target])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['prediction'] = clf_test.predict(data[features])\n",
        "data['plot'] = data[target]\n",
        "data.loc[data['process']==False, 'plot'] = data['prediction']"
      ],
      "metadata": {
        "id": "zZ_PoYptFSPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=data[time], y=data[target]))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "sz-WtlNkHl8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=data.loc[data['process']==True, time], y=data.loc[data['process']==True, 'plot'],mode='markers', name='real'))\n",
        "# fig.add_trace(go.Scatter(x=data[time], y=data['plot'], mode='markers', marker=dict(color=data['process'].astype('int'))))\n",
        "fig.add_trace(go.Scatter(x=data.loc[data['process']==False, time], y=data.loc[data['process']==False, 'plot'], mode='markers',name='filled'))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "hIBBAqdnFz4W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBpcpBSxyax1gMqj7K89K1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}