{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPc+NZtgoUBVSLfAA4m0O3T"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Подключаем ВАШ гугл-диск, как папку. В примере у меня в корне моего диска есть папка weather_data, где лежит файл boloto_2016.xlsx"
      ],
      "metadata": {
        "id": "2GWEPYB5SIen"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taO2EHOFQT84"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'NEE'#'co2_flux_full output'\n",
        "fileName = 'weather_data/for_filtering.xlsx'\n",
        "z_val = 5.5"
      ],
      "metadata": {
        "id": "KkU742PCyvsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.read_excel(os.path.join(\"/content/drive/MyDrive/\", fileName),\n",
        "                         sheet_name=None,          # загружаем все листы! Если надо конкретные, можно их прописать тут\n",
        "                         skiprows=None)#lambda x: x==1)  # Важный момент! В моем файле вторая строка идет без данных, поэтому я ее вот таким образом пропускаю, уберите эту опцию, если у вас все идет подряд.\n",
        "data = pd.concat(all_data.values(), ignore_index=True)"
      ],
      "metadata": {
        "id": "0MV8JdBiywpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['dateTime'] = pd.to_datetime(data['timestamp'],\n",
        "                                  format='%Y-%m-%d %H:%M:%S') #это шаблон того, как у нас выглядят наши данные.\n",
        "data.index = pd.DatetimeIndex(data['dateTime'])\n",
        "\n",
        "#data['dateTime'] = data['dateTime'].astype(str)\n",
        "# data = data.rename(columns={'Unnamed: 0': \"dateTime\"})\n",
        "# data['dateTime'] = pd.to_datetime(data['dateTime'],\n",
        "#                                   format='%d.%m.%Y %H:%M:%S') #это шаблон того, как у нас выглядят наши данные.\n",
        "# data.index = pd.DatetimeIndex(data['dateTime'])\n",
        "\n",
        "\n",
        "\n",
        "# data = data.rename(columns={'Unnamed: 0': \"dateTime\"})\n"
      ],
      "metadata": {
        "id": "hEX5jo8Byyko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['process_flag'] = np.invert(pd.isnull(data[target]))"
      ],
      "metadata": {
        "id": "N6f2mTRky4NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это наши функции, которые мы будем применять."
      ],
      "metadata": {
        "id": "QFpCZ9EOSkUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hampel_filter_pandas(input_series, window_size, n_sigmas=3):\n",
        "    k = 1.4826  # scale factor for Gaussian distribution\n",
        "    new_series = input_series.copy()\n",
        "    indices = None\n",
        "    # # helper lambda function\n",
        "    MAD = lambda x: np.median(np.abs(x - np.median(x)))\n",
        "\n",
        "    rolling_median = input_series.rolling(window=2 * window_size, center=True).median()\n",
        "    rolling_mad = k * input_series.rolling(window=2 * window_size, center=True).apply(MAD)\n",
        "    diff = np.abs(input_series - rolling_median)\n",
        "    print(len(diff.index), len(rolling_mad.index))\n",
        "\n",
        "    indices = diff > (n_sigmas * rolling_mad)  # list(np.argwhere(diff > (n_sigmas * rolling_mad)).flatten())\n",
        "    # #new_series[indices] = rolling_median[indices]\n",
        "\n",
        "    return new_series, indices[indices == True].index\n",
        "\n",
        "def mad_filter(input_df, target_col, z=5.5, fill_method='new'):\n",
        "    data_if = input_df[[target_col]].copy()\n",
        "    data_if['rolling_fill'] = data_if[target_col].rolling(10).mean()\n",
        "    data_if['plus_shift'] = data_if[target_col].shift(1)\n",
        "    null_index_plus = data_if['plus_shift'].isnull()\n",
        "    null_index_plus = null_index_plus[null_index_plus == True].index\n",
        "\n",
        "    data_if['minus_shift'] = data_if[target_col].shift(-1)\n",
        "    null_index_minus = data_if['minus_shift'].isnull()\n",
        "    null_index_minus = null_index_minus[null_index_minus == True].index\n",
        "\n",
        "    if fill_method == 'old':\n",
        "        data_if.loc[null_index_plus, 'plus_shift'] = data_if.loc[null_index_plus, target_col]\n",
        "        data_if.loc[null_index_minus, 'minus_shift'] = data_if.loc[null_index_minus, target_col]\n",
        "    else:\n",
        "        data_if.loc[null_index_plus, 'plus_shift'] = data_if.loc[null_index_plus, 'rolling_fill']\n",
        "        data_if.loc[null_index_minus, 'minus_shift'] = data_if.loc[null_index_minus, 'rolling_fill']\n",
        "\n",
        "    data_if['d_i'] = (data_if[target_col] - data_if['minus_shift']) - (data_if['plus_shift'] - data_if[target_col])\n",
        "\n",
        "    # print(key, pd.isna(data['d_i']).value_counts())\n",
        "    d_median = np.median(data_if.query('not (d_i != d_i)')['d_i'])\n",
        "    MAD = np.median(np.abs(data_if.query('not (d_i != d_i)')['d_i'] - d_median))\n",
        "\n",
        "    down_threshold = d_median - (z * MAD / 0.6745)\n",
        "    up_threshold = d_median + (z * MAD / 0.6745)\n",
        "\n",
        "    out_data = np.logical_or(data_if['d_i'] < down_threshold, data_if['d_i'] > up_threshold, data_if['d_i'].isna())\n",
        "    return out_data"
      ],
      "metadata": {
        "id": "0sDZYOKBtNhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.loc[data['process_flag']==True, 'filter_55'] = mad_filter(data.query('process_flag==True'), target, z=5.5, fill_method='new')\n",
        "data['filter_hampel'] = False\n",
        "rwn, indices = hampel_filter_pandas(data.query('process_flag==True')[target], window_size=10)\n",
        "data.loc[indices, 'filter_hampel'] = True\n",
        "\n",
        "# fig = px.scatter(data, x='dateTime', y=target, color='filter_55', title=f\"co2 MAD z={z_val}\")\n",
        "# # fig.write_html(f\"/content/drive/MyDrive/weather_data/co2_plot.html\")\n",
        "# fig.update_yaxes(type=\"log\")\n",
        "# fig.show()\n",
        "\n",
        "\n",
        "data['Outlier'] = False\n",
        "rwn, indices = hampel_filter_pandas(data.query('process_flag==True & filter_55==False')[target], window_size=10)\n",
        "data.loc[indices, 'Outlier'] = True\n",
        "data['Outlier'] = np.logical_or(data['Outlier'], data['filter_55'])\n",
        "fig = px.scatter(data.query('process_flag==True'), x='dateTime', y=target, color='Outlier', title=f'co2 Hampel after MAD {z_val}')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0itwCo-XzSHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir, out_filename =  os.path.split(os.path.join(\"/content/drive/MyDrive/\", fileName))\n",
        "out_filename = \"out_\"+out_filename\n",
        "data.to_csv(os.path.join(base_dir, out_filename))\n"
      ],
      "metadata": {
        "id": "32kYIhp2zHss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}